{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n",
      "Training data shape:  (50000, 3, 32, 32)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 3, 32, 32)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_CIFAR10\n",
    "\n",
    "cifar10_dir = \"cifar-10-batches-py\"\n",
    "\n",
    "\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_tra, y_tra, X_te, y_te = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "print('Training data shape: ', X_tra.shape)\n",
    "print('Training labels shape: ', y_tra.shape)\n",
    "print('Test data shape: ', X_te.shape)\n",
    "print('Test labels shape: ', y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_val = X_tra[40000:]\n",
    "y_val = y_tra[40000:]\n",
    "X_tra = X_tra[:40000]\n",
    "y_tra = y_tra[:40000]\n",
    "\n",
    "\n",
    "mini_batch = 625\n",
    "X_tra =  np.array(np.vsplit(X_tra,X_tra.shape[0]/mini_batch))\n",
    "y_tra =  np.array(np.split(y_tra,y_tra.shape[0]/mini_batch))\n",
    "X_val =  np.array(np.vsplit(X_val,X_val.shape[0]/mini_batch))\n",
    "y_val =  np.array(np.split(y_val,y_val.shape[0]/mini_batch))\n",
    "X_te =  np.array(np.vsplit(X_te,X_te.shape[0]/mini_batch))\n",
    "y_te =  np.array(np.split(y_te,y_te.shape[0]/mini_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  torch.Size([64, 625, 3, 32, 32])\n",
      "Training labels shape:  torch.Size([64, 625])\n",
      "Training data shape:  torch.Size([16, 625, 3, 32, 32])\n",
      "Training labels shape:  torch.Size([16, 625])\n",
      "Test data shape:  torch.Size([16, 625, 3, 32, 32])\n",
      "Test labels shape:  torch.Size([16, 625])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "classes = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "X_train = torch.from_numpy(X_tra)\n",
    "y_train = torch.from_numpy(y_tra)\n",
    "X_vali = torch.from_numpy(X_val)\n",
    "y_vali = torch.from_numpy(y_val)\n",
    "X_test = torch.from_numpy(X_te)\n",
    "y_test = torch.from_numpy(y_te)\n",
    "\n",
    "X_val = X_train[40000 : ]\n",
    "y_val = y_train[40000 : ]\n",
    "\n",
    "\n",
    "N,B,C,H,W=X_train.shape\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Training data shape: ', X_vali.shape)\n",
    "print('Training labels shape: ', y_vali.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 59.,  43.,  50.,  ..., 158., 152., 148.],\n",
      "         [ 16.,   0.,  18.,  ..., 123., 119., 122.],\n",
      "         [ 25.,  16.,  49.,  ..., 118., 120., 109.],\n",
      "         ...,\n",
      "         [208., 201., 198.,  ..., 160.,  56.,  53.],\n",
      "         [180., 173., 186.,  ..., 184.,  97.,  83.],\n",
      "         [177., 168., 179.,  ..., 216., 151., 123.]],\n",
      "\n",
      "        [[ 62.,  46.,  48.,  ..., 132., 125., 124.],\n",
      "         [ 20.,   0.,   8.,  ...,  88.,  83.,  87.],\n",
      "         [ 24.,   7.,  27.,  ...,  84.,  84.,  73.],\n",
      "         ...,\n",
      "         [170., 153., 161.,  ..., 133.,  31.,  34.],\n",
      "         [139., 123., 144.,  ..., 148.,  62.,  53.],\n",
      "         [144., 129., 142.,  ..., 184., 118.,  92.]],\n",
      "\n",
      "        [[ 63.,  45.,  43.,  ..., 108., 102., 103.],\n",
      "         [ 20.,   0.,   0.,  ...,  55.,  50.,  57.],\n",
      "         [ 21.,   0.,   8.,  ...,  50.,  50.,  42.],\n",
      "         ...,\n",
      "         [ 96.,  34.,  26.,  ...,  70.,   7.,  20.],\n",
      "         [ 96.,  42.,  30.,  ...,  94.,  34.,  34.],\n",
      "         [116.,  94.,  87.,  ..., 140.,  84.,  72.]]])\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=500, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3 , 10 ,5) #, padding='valid', input_shape=(C,H,W), activation='relu')\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20 , 5 ) #,padding='valid', input_shape=(20,H-5+1,W-5+1), activation='relu')\n",
    "        self.fc1 = nn.Linear(500, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 500)\n",
    "        #x = x.long()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7567148797337167e-06\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 3.7567148797337167e-06\n",
      "    momentum: 0.99\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import random\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# learning rate 값 찾기\n",
    "power = random.uniform(-3,-6)\n",
    "\n",
    "\n",
    "learning_rate = 10 ** power\n",
    "moment = 0.99\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.99)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(learning_rate)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acurr :  0.6096 // train acurr :  0.671025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6133 // train acurr :  0.66875 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6137 // train acurr :  0.66925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6133 // train acurr :  0.670425 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6124 // train acurr :  0.6713 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6118 // train acurr :  0.6736 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6115 // train acurr :  0.673175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.611 // train acurr :  0.6735 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6116 // train acurr :  0.672575 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6118 // train acurr :  0.673025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6119 // train acurr :  0.673325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6116 // train acurr :  0.6729 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.612 // train acurr :  0.673575 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6121 // train acurr :  0.6738 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6123 // train acurr :  0.6741 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6121 // train acurr :  0.6742 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6124 // train acurr :  0.67415 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6129 // train acurr :  0.674025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6124 // train acurr :  0.67425 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6129 // train acurr :  0.67425 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6129 // train acurr :  0.674175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6129 // train acurr :  0.6742 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6131 // train acurr :  0.674325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6125 // train acurr :  0.674475 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6129 // train acurr :  0.674775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6127 // train acurr :  0.6747 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6132 // train acurr :  0.67515 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6128 // train acurr :  0.6755 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6133 // train acurr :  0.675975 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6137 // train acurr :  0.675925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6138 // train acurr :  0.6759 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.6764 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.6767 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6133 // train acurr :  0.677025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6138 // train acurr :  0.6769 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6142 // train acurr :  0.677175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6142 // train acurr :  0.676975 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6137 // train acurr :  0.67695 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6141 // train acurr :  0.6772 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6138 // train acurr :  0.6771 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6139 // train acurr :  0.67725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6139 // train acurr :  0.67765 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.67775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.6778 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.677775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.677925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6145 // train acurr :  0.677775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.678225 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.678275 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6145 // train acurr :  0.67825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6143 // train acurr :  0.6788 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6145 // train acurr :  0.67895 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.67875 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6142 // train acurr :  0.67895 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6143 // train acurr :  0.6786 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6141 // train acurr :  0.678575 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6142 // train acurr :  0.67865 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6145 // train acurr :  0.678725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6141 // train acurr :  0.678825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.679025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6145 // train acurr :  0.67895 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6142 // train acurr :  0.679025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6136 // train acurr :  0.679175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.614 // train acurr :  0.6792 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.67955 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6143 // train acurr :  0.680125 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6143 // train acurr :  0.680625 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6142 // train acurr :  0.681 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6144 // train acurr :  0.681175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6152 // train acurr :  0.68115 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6147 // train acurr :  0.6811 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6148 // train acurr :  0.6813 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6152 // train acurr :  0.6815 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6155 // train acurr :  0.681575 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.681775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6152 // train acurr :  0.6818 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6148 // train acurr :  0.682325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6146 // train acurr :  0.682675 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.615 // train acurr :  0.683 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.615 // train acurr :  0.6833 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6156 // train acurr :  0.68345 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6157 // train acurr :  0.68325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6154 // train acurr :  0.683375 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6159 // train acurr :  0.68335 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.684 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.68365 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.683725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.616 // train acurr :  0.68375 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.683825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.683925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.68395 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.6841 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6165 // train acurr :  0.684225 // learning rate :  3.7567148797337167e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acurr :  0.6166 // train acurr :  0.684325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.6845 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6165 // train acurr :  0.68445 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.68435 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.616 // train acurr :  0.684325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6157 // train acurr :  0.684925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.68515 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.684975 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.685 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.6849 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6165 // train acurr :  0.685025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6159 // train acurr :  0.685275 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.685075 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.684625 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.684925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.684975 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.68535 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.685675 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.68595 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6167 // train acurr :  0.6864 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6168 // train acurr :  0.68665 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.6869 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6175 // train acurr :  0.687075 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6173 // train acurr :  0.687275 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6175 // train acurr :  0.687425 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6173 // train acurr :  0.687725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6181 // train acurr :  0.6879 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.688 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.68795 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.68845 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.688575 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.688825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6169 // train acurr :  0.689225 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6168 // train acurr :  0.6892 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6167 // train acurr :  0.689375 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.689675 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.6898 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6165 // train acurr :  0.689775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.689825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.689825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6165 // train acurr :  0.6896 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.690025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.69005 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.690175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.6902 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.690475 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.69075 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.690375 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6165 // train acurr :  0.690675 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.690925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6158 // train acurr :  0.690775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.69105 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6173 // train acurr :  0.691 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6174 // train acurr :  0.6914 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6176 // train acurr :  0.6912 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6182 // train acurr :  0.690975 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6186 // train acurr :  0.691225 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6188 // train acurr :  0.691225 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.618 // train acurr :  0.691275 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.69115 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6175 // train acurr :  0.690875 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6169 // train acurr :  0.69085 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6169 // train acurr :  0.691225 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.691425 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6174 // train acurr :  0.691375 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.69155 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.69175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6169 // train acurr :  0.691625 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.69145 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6159 // train acurr :  0.69165 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.691925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.692075 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.692625 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6173 // train acurr :  0.69315 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6175 // train acurr :  0.693475 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6174 // train acurr :  0.693725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6175 // train acurr :  0.693725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6174 // train acurr :  0.69435 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.69435 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.69455 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6173 // train acurr :  0.6946 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6175 // train acurr :  0.694675 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6168 // train acurr :  0.69485 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6179 // train acurr :  0.69505 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6172 // train acurr :  0.695525 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.69555 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6161 // train acurr :  0.69575 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6166 // train acurr :  0.6956 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6167 // train acurr :  0.6959 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6169 // train acurr :  0.695925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6167 // train acurr :  0.695825 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6171 // train acurr :  0.695975 // learning rate :  3.7567148797337167e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acurr :  0.6165 // train acurr :  0.6961 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6171 // train acurr :  0.6964 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6174 // train acurr :  0.69655 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.618 // train acurr :  0.696925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6179 // train acurr :  0.69705 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6178 // train acurr :  0.697175 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.617 // train acurr :  0.697325 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.697475 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6168 // train acurr :  0.6974 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.697725 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.69775 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6163 // train acurr :  0.697925 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.697975 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6164 // train acurr :  0.698025 // learning rate :  3.7567148797337167e-06\n",
      "val acurr :  0.6162 // train acurr :  0.697925 // learning rate :  3.7567148797337167e-06\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(200):   # 데이터셋을 수차례 반복합니다.\n",
    "    \n",
    "    total_tra = 0\n",
    "    correct_tra = 0\n",
    "    running_loss = 0.0\n",
    "    # train 데이터 이용한 학습\n",
    "    for i in range(X_train.shape[0]):\n",
    "        inputs = X_train[i].to(device)\n",
    "        labels = y_train[i].to(device)\n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_tra += labels.shape[0]\n",
    "        correct_tra += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계를 출력합니다.\n",
    "        running_loss += loss.item()\n",
    "        if i % 8 == 7:    # print every 2000 mini-batches\n",
    "            #print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss /batch_size))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    total_val = 0\n",
    "    correct_val = 0\n",
    "    for i in range(X_vali.shape[0]):\n",
    "        inputs = X_vali[i].to(device)\n",
    "        labels = y_vali[i].to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_val += labels.shape[0]\n",
    "        correct_val += (predicted == labels).sum().item()\n",
    "    print(\"val acurr : \", correct_val/total_val ,end = \" // \")\n",
    "    print(\"train acurr : \", correct_tra/total_tra ,end = \" // \")\n",
    "    print(\"learning rate : \", learning_rate)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans :  ship\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans :  car\n",
      "predict : car car\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    #npimg = img.numpy()\n",
    "    #print(img)\n",
    "    plt.imshow(np.transpose(img.astype(\"int\"), (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "images=[]\n",
    "for k in range(2):\n",
    "    i = random.randint(1,64)\n",
    "    j = random.randint(1,625)\n",
    "    images.append(X_tra[i][j])\n",
    "    imshow(X_tra[i][j])\n",
    "    print(\"ans : \",classes[y_tra[i][j]])\n",
    "\n",
    "images=torch.tensor(images).to(device)\n",
    "output = net(images)\n",
    "_, predicted = torch.max(output, 1)\n",
    "print(\"predict :\" ,classes[predicted[0]],classes[predicted[1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 61.380000 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(X_test.shape[0]):\n",
    "        images = X_test[i].to(device)\n",
    "        labels = y_test[i].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.shape[0]\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class :  plane  //  acur :  0.632\n",
      "class :  car  //  acur :  0.725\n",
      "class :  bird  //  acur :  0.5\n",
      "class :  cat  //  acur :  0.425\n",
      "class :  deer  //  acur :  0.518\n",
      "class :  dog  //  acur :  0.496\n",
      "class :  frog  //  acur :  0.771\n",
      "class :  horse  //  acur :  0.628\n",
      "class :  ship  //  acur :  0.76\n",
      "class :  truck  //  acur :  0.683\n"
     ]
    }
   ],
   "source": [
    "class_correct = np.zeros(10)\n",
    "class_total = np.zeros(10)\n",
    "with torch.no_grad():\n",
    "    for i in range(X_test.shape[0]):\n",
    "        images = X_test[i].to(device)\n",
    "        labels = y_test[i].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for j in range(labels.shape[0]):\n",
    "            class_total[labels[j]] += 1\n",
    "            if labels[j] == predicted[j]:\n",
    "                class_correct[labels[j]] += 1\n",
    "\n",
    "for i in range(10):         \n",
    "    print(\"class : \",classes[i] , \" // \", \"acur : \" ,class_correct[i]/class_total[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
